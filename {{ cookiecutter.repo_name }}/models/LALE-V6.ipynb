{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":71485,"databundleVersionId":8059942,"sourceType":"competition"},{"sourceId":8354103,"sourceType":"datasetVersion","datasetId":4963814},{"sourceId":8695083,"sourceType":"datasetVersion","datasetId":5214346}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport datetime as datetime\nfrom datetime import datetime\n\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras import mixed_precision\nfrom tensorflow.keras.regularizers import l2\n\npolicy = mixed_precision.Policy('mixed_float16')\nmixed_precision.set_global_policy(policy)\n\nprint(tf.__version__)\n\nfrom imblearn.over_sampling import SMOTE\nimport random\nimport os\n\nSEED = 42\nmax_length = 3000\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\nos.environ['PYTHONHASHED'] = str(SEED)\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nimport os\nos.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n\n\ntf.keras.backend.clear_session()\n\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    except RuntimeError as e:\n        print(e)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv')\ndf_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv')\ndf_sub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['score'] = df_train['score'] - 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_aug = df_train.copy(deep=True)\ndf_aug['full_text'] = np.NaN\nprint(df_aug.head())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gensim\nfrom gensim.models import KeyedVectors\n\nglove_file = '/kaggle/input/glove6b300d/glove.6B.300d.txt'\n\nprint(\"Loading word vectors\\n\")\nload_start = datetime.now()\nprint(load_start)\n\nword_vectors = KeyedVectors.load_word2vec_format(fname=glove_file, binary=False, unicode_errors='ignore', no_header=True, limit=400000)\n\nload_end = datetime.now()\nprint(load_end)\nprint(\"Word vectors loaded\\n\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Precomputing similar words\\n\")\ncomp_start = datetime.now()\nprint(comp_start)\n\nsimilar_words_dict = {word: word_vectors.most_similar(word, topn=1)[0][0] for word in word_vectors.index_to_key}\n\ncomp_end = datetime.now()\nprint(comp_end)    \nprint(\"Precompute complete\\n\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from multiprocessing import Pool\n\ndef augment_text_with_glove(text):\n    augmented_text = []\n    for word in text.split():\n        if word in similar_words_dict:\n            augmented_text.append(similar_words_dict[word])\n        else:\n            augmented_text.append(word)\n    return ' '.join(augmented_text)\n\ndef parallel_augment_texts(texts, num_workers=4):\n    with Pool(num_workers) as pool:\n        augmented_texts = list(pool.imap(augment_text_with_glove, texts))\n    return augmented_texts\n\nprint(\"Starting data augmentation now\\n\")\naug_start = datetime.now()\nprint(\"Augmentation start time is: \", aug_start)\n\ndf_aug['full_text'] = parallel_augment_texts(df_train['full_text'].tolist())\n\naug_end = datetime.now()\nprint(\"\\nAugmentation end time is: \", aug_end)\nprint(\"\\nData augmentation complete\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_fin = pd.concat([df_train, df_aug], axis=0)\nlen(df_train_fin)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit as sss\n\nsplits = sss(n_splits=1, test_size=0.1, random_state=42)\n\nfor train_index, test_index in splits.split(df_train_fin['full_text'], df_train_fin['score']):\n    X_train, X_test = df_train_fin['full_text'][train_index], df_train_fin['full_text'][test_index]\n    y_train, y_test = df_train_fin['score'][train_index], df_train_fin['score'][test_index]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"X_train:\", X_train)\nprint(\"X_test:\", X_test)\nprint(\"y_train:\", y_train)\nprint(\"y_test:\", y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import spacy\nfrom concurrent.futures import ProcessPoolExecutor\n\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Function to apply lemmatization\ndef lemmatize_text(text):\n    doc = nlp(text)\n    return \" \".join([token.lemma_ for token in doc if not token.is_stop])\n\ndef process_chunk(chunk):\n    return chunk.apply(lemmatize_text)\n\ndef parallel_lemmatize(data, num_processes):\n    chunks = np.array_split(data, num_processes)\n    with ProcessPoolExecutor(max_workers=num_processes) as executor:\n        results = list(executor.map(process_chunk, chunks))\n    return pd.concat(results, ignore_index=True)\n\nprint(\"\\nApplying lemmatization now\")\nlem_start = datetime.now()\nprint(\"\\nLemmatization start time is: \", lem_start)\n\nnum_processes = 8\nX_train = parallel_lemmatize(X_train, num_processes)\n\nlem_end = datetime.now()\nprint(\"\\nLemmatization end time is: \", lem_end)\nprint(\"\\nData lemmatization complete\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = X_train.tolist()\ny_train = y_train.tolist()\nX_test = X_test.tolist()\ny_test = y_test.tolist()\n\nX_train_nn = X_train\ny_train_nn = y_train\nX_test_nn = X_test\ny_test_nn = y_test\n\ntfidf_vectorizer = TfidfVectorizer(max_features=100000, analyzer='word', stop_words='english')\nX_train = tfidf_vectorizer.fit_transform(X_train).toarray()\nX_test = tfidf_vectorizer.transform(X_test).toarray()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_text = df_sub['full_text'].tolist()\nsub_X = tfidf_vectorizer.transform(sub_text).toarray()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Expected feature shape: {X_train.shape[1]}\")\nprint(f\"Sub_X feature shape: {sub_X.shape[1]}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.metrics import accuracy_score\n\nxgb_model = xgb.XGBClassifier(n_jobs=-1, device='cuda', grow_policy='lossguide', colsample_bytree=0.7, booster='gbtree', n_estimators=500, subsample=0.7,  \n                              random_state=SEED)\nxgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose = False) # , early_stopping_rounds=100, max_depth=7, scale_pos_weight=1, learning_rate=0.01, reg_alpha=0, min_child_weight=1, gamma=0, reg_lambda=1, objective='multi:softprob',","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_preds = xgb_model.predict(X_test) + 1\nxgb_accuracy = accuracy_score(y_test, xgb_preds)\nprint(f\"XGBoost Accuracy: {xgb_accuracy}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_xgb_preds = xgb_model.predict(sub_X) + 1\nprint(sub_xgb_preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf_model = RandomForestClassifier(n_estimators=1000, bootstrap=True, oob_score=True, max_features='sqrt', n_jobs=-1, verbose=1, random_state=SEED)\nrf_model.fit(X_train, y_train)\n\nrf_preds = rf_model.predict(X_test) + 1\nrf_accuracy = accuracy_score(y_test, rf_preds)\nprint(f\"Random Forest Accuracy: {rf_accuracy}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_rf_preds = rf_model.predict(sub_X) + 1\nprint(sub_rf_preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_nn\nlabels = y_train_nn\nX_test_nn\nval_labels = y_test_nn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tokenizers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tokenizers import Tokenizer, models, pre_tokenizers, trainers, normalizers\n# Initialize and train a BPE tokenizer\ntokenizer = Tokenizer(models.BPE())\ntokenizer.normalizer = normalizers.Sequence([normalizers.NFKC(), normalizers.Lowercase()])\ntokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\ntrainer = trainers.BpeTrainer(vocab_size=400000, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\ntokenizer.train_from_iterator(X_train_nn, trainer)\ntokenizer.save(\"tokenizer.json\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer.from_file(\"tokenizer.json\")\ntokenized_texts = [tokenizer.encode(text).ids for text in X_train_nn]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_tokenized_texts = [tokenizer.encode(val_text).ids for val_text in X_test_nn]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pad the tokenized texts\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nmax_length = 3000\npadded_texts = pad_sequences(tokenized_texts, maxlen=max_length, padding='post')\n\nval_padded_texts = pad_sequences(val_tokenized_texts, maxlen=max_length, padding='post')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a TensorFlow dataset\ndataset = tf.data.Dataset.from_tensor_slices((padded_texts, labels))\ndataset = dataset.shuffle(len(X_train_nn)).batch(128)\n\nval_dataset = tf.data.Dataset.from_tensor_slices((val_padded_texts, val_labels))\nval_dataset = val_dataset.batch(128)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.regularizers import l2\n\ntf.keras.backend.clear_session()\n\nnn_model = tf.keras.Sequential([\ntf.keras.layers.Input(shape=(max_length,)),\ntf.keras.layers.Embedding(input_dim=400000, output_dim=128), \ntf.keras.layers.Conv1D(128, 5, activation='relu', kernel_regularizer=l2(1)),\ntf.keras.layers.BatchNormalization(),    \ntf.keras.layers.Dropout(0.4),\ntf.keras.layers.Dense(64, activation='relu', kernel_regularizer=l2(1)),\ntf.keras.layers.Dropout(0.4),\ntf.keras.layers.Dense(64, activation='relu', kernel_regularizer=l2(1)),\ntf.keras.layers.Dropout(0.4),\ntf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True, kernel_regularizer=l2(1))),\ntf.keras.layers.LSTM(32, return_sequences=True, kernel_regularizer=l2(1)),    \ntf.keras.layers.GlobalMaxPooling1D(),\ntf.keras.layers.Dense(6, activation='softmax')\n])\n\nnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# checkpoint_filepath = 'best_model.keras'\n# model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n#     filepath=checkpoint_filepath,\n#     save_weights_only=False,\n#     monitor='val_accuracy',\n#     mode='max',\n#     save_best_only=True\n# )\n\nnn_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = nn_model.fit(dataset, epochs=10, validation_data=val_dataset) # , callbacks=[model_checkpoint_callback]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot utility\ndef plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.plot(history.history['val_'+string])\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()\n\n# Plot the accuracy and results \nplot_graphs(history, \"accuracy\")\nplot_graphs(history, \"loss\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# best_model = tf.keras.models.load_model('best_model.keras')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_texts = df_sub['full_text'].tolist()\ntokenized_pred_texts = [tokenizer.encode(pred_text).ids for pred_text in pred_texts]\npadded_pred_texts = pad_sequences(tokenized_pred_texts, maxlen=max_length, padding='post')\nnumpy_pred_texts = np.array(padded_pred_texts)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = nn_model.predict(numpy_pred_texts)\npreds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_preds = np.argmax(preds, axis=1)\nres_lst = max_preds + 1\nres_lst","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.stats import mode\n\ncombined_preds = np.array([sub_xgb_preds, sub_rf_preds, res_lst])\nfinal_preds = mode(combined_preds, axis=0)[0].flatten()\n\nfinal_preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub_fin = pd.DataFrame()\ndf_sub_fin['essay_id'] = df_sub['essay_id']\ndf_sub_fin","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub_fin['score'] = final_preds.transpose()\ndf_sub_fin['score'] = df_sub_fin['score'].astype('int')\ndf_sub_fin","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub_fin.to_csv('submission.csv', header=True, index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\n\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# texts = df_train['full_text'].tolist()\n# labels = df_train['score'].tolist()\n\n# tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=35000)\n# X = tfidf_vectorizer.fit_transform(texts).toarray()\n# y = labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# text_sub = df_sub['full_text'].tolist()\n# X_sub = tfidf_vectorizer.transform(text_sub).toarray()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(f\"Expected feature shape: {X_train.shape[1]}\")\n# print(f\"X_sub feature shape: {X_sub.shape[1]}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import xgboost as xgb\n# from sklearn.metrics import accuracy_score\n\n# xgb_model = xgb.XGBClassifier(n_jobs=-1, device='cuda', grow_policy='lossguide', colsample_bytree=0.8)\n# xgb_model.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# xgb_preds = xgb_model.predict(X_test) + 1\n# xgb_accuracy = accuracy_score(y_test, xgb_preds)\n# print(f\"XGBoost Accuracy: {xgb_accuracy}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sub_xgb_preds = xgb_model.predict(X_sub) + 1\n# print(sub_xgb_preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.ensemble import RandomForestClassifier\n\n# rf_model = RandomForestClassifier()\n# rf_model.fit(X_train, y_train)\n\n# rf_preds = rf_model.predict(X_test) + 1\n# rf_accuracy = accuracy_score(y_test, rf_preds)\n# print(f\"Random Forest Accuracy: {rf_accuracy}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sub_rf_preds = rf_model.predict(X_sub) + 1\n# print(sub_rf_preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import numpy as np\n\n# X_train_reshaped = np.expand_dims(X_train, axis=-1)\n# X_test_reshaped = np.expand_dims(X_test, axis=-1)\n\n# print(X_train_reshaped.shape)\n# print(X_test_reshaped.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y_train = np.array(y_train)\n# y_test = np.array(y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import tensorflow as tf\n# from tensorflow.keras.models import Sequential\n# from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv1D, Bidirectional, LSTM, GlobalMaxPooling1D\n# from tensorflow.keras.regularizers import l2\n\n# tf.keras.backend.clear_session()\n\n# nn_model = Sequential([\n#     Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train_reshaped.shape[1], 1)),\n#     Dropout(0.4),\n#     LSTM(32, return_sequences=True, kernel_regularizer=l2(0.01)),\n#     Bidirectional(LSTM(32, return_sequences=True, kernel_regularizer=l2(0.01))),\n#     GlobalMaxPooling1D(),    \n#     Flatten(),\n#     Dense(64, activation='relu'),\n#     Dropout(0.4),        \n#     Dense(6, activation='softmax')\n# ])\n\n# nn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# nn_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# nn_model.fit(X_train_reshaped, y_train, epochs=15, batch_size=64, validation_data=(X_test_reshaped, y_test))\n\n# nn_preds = nn_model.predict(X_test_reshaped)\n# nn_preds_classes = nn_preds.argmax(axis=1) + 1\n# nn_accuracy = accuracy_score(y_test, nn_preds_classes)\n# print(f\"Neural Network Accuracy: {nn_accuracy}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sub_X_reshaped = np.expand_dims(X_sub, axis=-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sub_nn_preds = nn_model.predict(sub_X_reshaped)\n# sub_nn_pred_classes = sub_nn_preds.argmax(axis=1) + 1\n# print(sub_nn_pred_classes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from scipy.stats import mode\n\n# combined_preds = np.array([sub_xgb_preds, sub_rf_preds, sub_nn_pred_classes])\n# final_preds = mode(combined_preds, axis=0)[0].flatten()\n\n# final_preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_sub_fin = pd.DataFrame()\n# df_sub_fin['essay_id'] = df_sub['essay_id']\n# df_sub_fin","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_sub_fin['score'] = final_preds.transpose()\n# df_sub_fin['score'] = df_sub_fin['score'].astype('int')\n# df_sub_fin","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_sub_fin.to_csv('submission.csv', header=True, index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\n\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# len(df_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_aug = df_train.copy(deep=True)\n# df_aug['full_text'] = np.NaN","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(df_aug)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import pickle\n\n# with open('/kaggle/input/similarwordsdict/similar_words_dict.pkl', 'rb') as fp:\n#     similar_words_dict = pickle.load(fp)\n#     print('similar_words_dict loaded')    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from multiprocessing import Pool\n\n# def augment_text_with_glove(text):\n#     augmented_text = []\n#     for word in text.split():\n#         if word in similar_words_dict:\n#             augmented_text.append(similar_words_dict[word])\n#         else:\n#             augmented_text.append(word)\n#     return ' '.join(augmented_text)\n\n# def parallel_augment_texts(texts, num_workers=4):\n#     with Pool(num_workers) as pool:\n#         augmented_texts = list(pool.imap(augment_text_with_glove, texts))\n#     return augmented_texts\n\n# print(\"Starting data augmentation now\\n\")\n# aug_start = datetime.now()\n# print(\"Augmentation start time is: \", aug_start)\n\n# df_aug['full_text'] = parallel_augment_texts(df_train['full_text'].tolist())\n\n# aug_end = datetime.now()\n# print(\"\\nAugmentation end time is: \", aug_end)\n# print(\"\\nData augmentation complete\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_train_fin = pd.concat([df_train, df_aug], axis=0)\n# len(df_train_fin)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_train_fin['score'] = df_train_fin['score'] - 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_train_fin['full_text'].map(len).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.model_selection import StratifiedShuffleSplit as sss\n# SEED = 42\n# splits = sss(n_splits=1, test_size=0.1, random_state=SEED)\n\n# for train_index, test_index in splits.split(df_train_fin, df_train_fin['score']):\n#     train_set = df_train_fin.iloc[train_index]\n#     test_set = df_train_fin.iloc[test_index]\n\n# print(\"Train set indices:\", train_set.index)\n# print(\"Test set indices:\", test_set.index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import spacy\n# # Load the English tokenizer, tagger, parser, NER, and word vectors\n# nlp = spacy.load(\"en_core_web_sm\")\n\n# # Function to apply lemmatization\n# def lemmatize_text(text):\n#     doc = nlp(text)\n#     return \" \".join([token.lemma_ for token in doc])\n\n# from concurrent.futures import ProcessPoolExecutor\n\n# def process_chunk(chunk):\n#     return chunk.apply(lemmatize_text)\n\n# def parallel_lemmatize(data, num_processes):\n#     chunks = np.array_split(data, num_processes)\n#     with ProcessPoolExecutor(max_workers=num_processes) as executor:\n#         results = list(executor.map(process_chunk, chunks))\n#     return pd.concat(results, ignore_index=True)\n\n# print(\"\\nApplying lemmatization now\")\n# lem_start = datetime.now()\n# print(\"\\nLemmatization start time is: \", lem_start)\n\n# num_processes = 8\n# train_set['full_text'] = parallel_lemmatize(train_set['full_text'], num_processes)\n\n# lem_end = datetime.now()\n# print(\"\\nLemmatization end time is: \", lem_end)\n# print(\"\\nData lemmatization complete\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# texts = train_set['full_text'].tolist()\n# labels = train_set['score'].tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# val_texts = test_set['full_text'].tolist()\n# val_labels = test_set['score'].tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tokenizers import Tokenizer, models, pre_tokenizers, trainers, normalizers\n# tokenizer = Tokenizer(models.BPE())\n# tokenizer.normalizer = normalizers.Sequence([normalizers.NFKC(), normalizers.Lowercase()])\n# tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n# trainer = trainers.BpeTrainer(vocab_size=400000, special_tokens = [\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n# tokenizer.train_from_iterator(texts, trainer)\n# tokenizer.save(\"tokenizer.json\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenizer = Tokenizer.from_file(\"tokenizer.json\")\n# tokenized_texts = [tokenizer.encode(text).ids for text in texts]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# val_tokenized_texts = [tokenizer.encode(val_text).ids for val_text in val_texts]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# max_length = 3000\n# padded_texts = pad_sequences(tokenized_texts, maxlen=max_length, padding='post')\n\n# val_padded_texts = pad_sequences(val_tokenized_texts, maxlen=max_length, padding='post')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from imblearn.over_sampling import SMOTE\n\n# smote = SMOTE(random_state=SEED)\n# padded_sequences_res, labels_res = smote.fit_resample(padded_texts, labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(\"Original class distribution:\\n\", pd.Series(labels).value_counts())\n# print(\"Resampled class distribution:\\n\", pd.Series(labels_res).value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dataset = tf.data.Dataset.from_tensor_slices((padded_sequences_res, labels_res))\n# dataset = dataset.shuffle(len(texts), seed=SEED).batch(128)\n\n# val_dataset = tf.data.Dataset.from_tensor_slices((val_padded_texts, val_labels))\n# val_dataset = val_dataset.batch(128)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tf.keras.backend.clear_session()\n\n# model = tf.keras.Sequential([\n#     tf.keras.layers.Input(shape=(max_length,)),\n#     tf.keras.layers.Embedding(input_dim=400000, output_dim=128),\n#     tf.keras.layers.Conv1D(128, 5, activation='relu'),\n#     tf.keras.layers.Dropout(0.4),\n#     tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n#     tf.keras.layers.Dropout(0.4),\n#     tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n#     tf.keras.layers.Dropout(0.4),\n#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True, kernel_regularizer=l2(0.01))),\n#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True, kernel_regularizer=l2(0.01))),\n#     tf.keras.layers.GlobalMaxPooling1D(),\n# #     Attention(),\n#     tf.keras.layers.Dense(6, activation='softmax')                      \n# ])\n\n# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tf.keras.backend.clear_session()\n\n# history = model.fit(dataset, epochs=40, validation_data=val_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n\n# # Plot utility\n# def plot_graphs(history, string):\n#   plt.plot(history.history[string])\n#   plt.plot(history.history['val_'+string])\n#   plt.xlabel(\"Epochs\")\n#   plt.ylabel(string)\n#   plt.legend([string, 'val_'+string])\n#   plt.show()\n\n# # Plot the accuracy and results \n# plot_graphs(history, \"accuracy\")\n# plot_graphs(history, \"loss\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_test = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv')\n# df_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pred_texts = df_test['full_text'].tolist()\n# tokenized_pred_texts = [tokenizer.encode(pred_text).ids for pred_text in pred_texts]\n# padded_pred_texts = pad_sequences(tokenized_pred_texts, maxlen=max_length, padding='post')\n# numpy_pred_texts = np.array(padded_pred_texts)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preds = model.predict(numpy_pred_texts)\n# preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# max_preds = np.argmax(preds, axis=1)\n# res_lst = max_preds + 1\n# res_lst","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_sub = pd.DataFrame()\n# df_sub['essay_id'] = df_test['essay_id']\n# df_sub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_sub['score'] = res_lst.transpose()\n# df_sub['score'] = df_sub['score'].astype('int')\n# df_sub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_sub.to_csv('submission.csv', header=True, index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}